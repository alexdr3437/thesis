
@inproceedings{schmidt_actor-critic_2014,
	title = {Actor-critic design using echo state networks in a simulated quadruped robot},
	doi = {10.1109/IROS.2014.6942862},
	abstract = {In recent years, several studies proposed the application of echo state networks (ESN) to adaptive reinforcement learning schemes for the control of artificial autonomous agents. Especially the actor-critic design (ACD) is a promising candidate for robotic systems with continuous state and action spaces, as was demonstrated in several studies using simple wheeled robots. In the present work, we investigate applicability of this learning framework to more complex robotic systems, namely a quadruped running robot with rich dynamics. New challenges and questions arise, such as the nontrivial mapping of actions to the resulting behavior.},
	booktitle = {2014 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Schmidt, Nico M. and Baumgartner, Matthias and Pfeifer, Rolf},
	month = sep,
	year = {2014},
	note = {ISSN: 2153-0866},
	pages = {2224--2229},
}

@inproceedings{schulman_trust_2015,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {https://proceedings.mlr.press/v37/schulman15.html},
	abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	language = {en},
	urldate = {2022-01-31},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {1889--1897},
}

@article{galloway_torque_2015,
	title = {Torque {Saturation} in {Bipedal} {Robotic} {Walking} {Through} {Control} {Lyapunov} {Function}-{Based} {Quadratic} {Programs}},
	volume = {3},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2015.2419630},
	abstract = {This paper presents a novel method to address the actuator saturation for nonlinear hybrid systems by directly incorporating user-defined input bounds in a controller design. In particular, we consider the application of bipedal walking and show that our method [based on a quadratic programming (QP) implementation of a control Lyapunov function (CLF)-based controller] enables a gradual performance degradation while still continuing to walk under increasingly stringent input bounds. We draw on our previous work, which has demonstrated the effectiveness of the CLF-based controllers for stabilizing periodic gaits for biped walkers. This paper presents a framework, which results in more effective handling of control saturations and provides a means for incorporating a whole family of user-defined constraints into the online computation of a CLF-based controller. This paper concludes with an experimental validation of the main results on the bipedal robot MABEL, demonstrating the usefulness of the QP-based CLF approach for real-time robotic control.},
	journal = {IEEE Access},
	author = {Galloway, Kevin and Sreenath, Koushil and Ames, Aaron D. and Grizzle, Jessy W.},
	year = {2015},
	note = {Conference Name: IEEE Access},
	pages = {323--332},
}

@article{defazio_learning_2021,
	title = {Learning {Quadruped} {Locomotion} {Policies} with {Reward} {Machines}},
	url = {http://arxiv.org/abs/2107.10969},
	abstract = {Legged robots have been shown to be effective in navigating unstructured environments. Although there has been much success in learning locomotion policies for quadruped robots, there is little research on how to incorporate human knowledge to facilitate this learning process. In this paper, we demonstrate that human knowledge in the form of LTL formulas can be applied to quadruped locomotion learning within a Reward Machine (RM) framework. Experimental results in simulation show that our RM-based approach enables easily deﬁning diverse locomotion styles, and efﬁciently learning locomotion policies of the deﬁned styles.},
	language = {en},
	urldate = {2022-01-31},
	journal = {arXiv:2107.10969 [cs]},
	author = {DeFazio, David and Zhang, Shiqi},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.10969},
}

@inproceedings{liu_robot_2021,
	title = {Robot {Reinforcement} {Learning} on the {Constraint} {Manifold}},
	url = {https://openreview.net/forum?id=zwo1-MdMl1P},
	abstract = {Reinforcement learning in robotics is extremely challenging due to many practical issues, including safety, mechanical constraints, and wear and tear. Typically, these issues are not considered in...},
	language = {en},
	urldate = {2022-01-31},
	author = {Liu, Puze and Tateo, Davide and Ammar, Haitham Bou and Peters, Jan},
	month = jun,
	year = {2021},
}

@inproceedings{ma_bipedal_2017,
	title = {Bipedal {Robotic} {Running} with {DURUS}-{2D}: {Bridging} the {Gap} between {Theory} and {Experiment}},
	shorttitle = {Bipedal {Robotic} {Running} with {DURUS}-{2D}},
	doi = {10.1145/3049797.3049823},
	abstract = {Bipedal robotic running remains a challenging benchmark in the field of control and robotics because of its highly dynamic nature and necessarily underactuated hybrid dynamics. Previous results have achieved bipedal running experimentally with a combination of theoretical results and heuristic application thereof. In particular, formal analysis of the hybrid system stability is given based on a theoretical model, but due to the gap between theoretical concepts and experimental reality, extensive tuning is necessary to achieve experimental success. In this paper, we present a formal approach to bridge this gap, starting from theoretical gait generation to a provably stable control implementation, resulting in bipedal robotic running. We first use a large-scale optimization to generate an energy-efficient running gait, subject to hybrid zero dynamics conditions and feasibility constraints which incorporate practical limitations of the robot model based on physical conditions. The stability of the gait is formally guaranteed in the hybrid system model with an input to state stability (ISS) based control law. This implementation improves the stability under practical control limitations of the system. Finally, the methodology is experimentally realized on the planar spring-legged bipedal robot, DURUS-2D, resulting in sustainable running at 1.75m/s. The paper, therefore, presents a formal method that takes the first step toward bridging the gap between theory and experiment.},
	author = {Ma, Wen-Loong and Kolathaya, Shishir and Ambrose, Eric and Hubicki, Christian and Ames, Aaron},
	month = apr,
	year = {2017},
}

@article{han_reinforcement_2021,
	title = {Reinforcement learning control of constrained dynamic systems with uniformly ultimate boundedness stability guarantee},
	volume = {129},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109821002090},
	doi = {10.1016/j.automatica.2021.109689},
	abstract = {Reinforcement learning (RL) is promising for complicated stochastic nonlinear control problems. Without using a mathematical model, an optimal controller can be learned from data evaluated by certain performance criteria through trial-and-error. However, the data-based learning approach is notorious for not guaranteeing stability, which is the most fundamental property for any control system. In this paper, the classic Lyapunov’s method is explored to analyze the uniformly ultimate boundedness stability (UUB) solely based on data without using a mathematical model. It is further shown how RL with UUB guarantee can be applied to control dynamic systems with safety constraints. Based on the theoretical results, both off-policy and on-policy learning algorithms are proposed respectively. As a result, optimal controllers can be learned to guarantee UUB of the closed-loop system both at convergence and during learning. The proposed algorithms are evaluated on a series of robotic continuous control tasks with safety constraints. In comparison with the existing RL algorithms, the proposed method can achieve superior performance in terms of maintaining safety. As a qualitative evaluation of stability, our method shows impressive resilience even in the presence of external disturbances.},
	language = {en},
	urldate = {2022-01-31},
	journal = {Automatica},
	author = {Han, Minghao and Tian, Yuan and Zhang, Lixian and Wang, Jun and Pan, Wei},
	month = jul,
	year = {2021},
	pages = {109689},
}

@inproceedings{dai_lyapunov-stable_2021,
	title = {Lyapunov-stable neural-network control},
	isbn = {978-0-9923747-7-8},
	url = {http://www.roboticsproceedings.org/rss17/p063.pdf},
	doi = {10.15607/RSS.2021.XVII.063},
	abstract = {Deep learning has had a far reaching impact in robotics. Speciﬁcally, deep reinforcement learning algorithms have been highly effective in synthesizing neural-network controllers for a wide range of tasks. However, despite this empirical success, these controllers still lack theoretical guarantees on their performance, such as Lyapunov stability (i.e., all trajectories of the closed-loop system are guaranteed to converge to a goal state under the control policy). This is in stark contrast to traditional model-based controller design, where principled approaches (like LQR) can synthesize stable controllers with provable guarantees. To address this gap, we propose a generic method to synthesize a Lyapunov-stable neural-network controller, together with a neural-network Lyapunov function to simultaneously certify its stability. Our approach formulates the Lyapunov condition veriﬁcation as a mixed-integer linear program (MIP). Our MIP veriﬁer either certiﬁes the Lyapunov condition, or generates counter examples that can help improve the candidate controller and the Lyapunov function. We also present an optimization program to compute an inner approximation of the region of attraction for the closed-loop system. We apply our approach to robots including an inverted pendulum, a 2D and a 3D quadrotor, and showcase that our neural-network controller outperforms a baseline LQR controller. The code is open sourced at https://github.com/StanfordASL/neural-network-lyapunov.},
	language = {en},
	urldate = {2022-01-31},
	booktitle = {Robotics: {Science} and {Systems} {XVII}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Dai, Hongkai and Landry, Benoit and Yang, Lujie and Pavone, Marco and Tedrake, Russ},
	month = jul,
	year = {2021},
}

@misc{coumans_pybullet_2017,
	title = {Pybullet, a python module for physics simulation in robotics, games and machine learning},
	url = {https://pybullet.org/},
	abstract = {Real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc.},
	author = {Coumans, Erwin and Bai, Yunfei},
	year = {2017},
}

@article{ma_coupled_2021,
	title = {Coupled {Control} {Lyapunov} {Functions} for {Interconnected} {Systems}, {With} {Application} to {Quadrupedal} {Locomotion}},
	volume = {6},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9376602/},
	doi = {10.1109/LRA.2021.3065174},
	abstract = {This letter addresses the problem of formally guaranteeing the stability of interconnected systems with local controllers with a view toward stabilizing quadrupeds viewed as coupled bipeds. In particular, we present a novel framework that views general rigid-body systems as a collection of lower-dimensional systems that are coupled via reaction forces. Stabilizing the corresponding coupled control system can thus be addressed by stabilizing each subsystem coupled through the passive dynamics. The main results of the letter are stability conditions that guarantee convergence for each control subsystem by formulating coupled control Lyapunov functions (CCLFs) using the notion of input-to-state stability (ISS). This theoretical result is illustrated via a simple cart-pole example, where exponential stability is obtained. Next, building on previous results where an 18-DOF quadrupedal robot is decomposed into two interconnected bipedal systems for efﬁcient periodic gait generation, we design model-free quadratic programs (QPs) using the CCLFs to stabilize the continuous dynamics and thus achieve experimental walking and simulated hopping and running on the Vision 60 quadrupedal robot.},
	language = {en},
	number = {2},
	urldate = {2022-01-31},
	journal = {IEEE Robotics and Automation Letters},
	author = {Ma, Wen-Loong and Csomay-Shanklin, Noel and Kolathaya, Shishir and Hamed, Kaveh Akbari and Ames, Aaron D.},
	month = apr,
	year = {2021},
	pages = {3761--3768},
}

@inproceedings{nguyen_3d_2016,
	address = {Las Vegas, NV, USA},
	title = {{3D} dynamic walking on stepping stones with control barrier functions},
	doi = {10.1109/CDC.2016.7798370},
	abstract = {3D dynamical walking subject to precise footstep placements is crucial for navigating real world terrain with discrete footholds. We present a novel methodology that combines control Lyapunov functions-to achieve periodic walking-and control Barrier functions-to enforce strict constraints on step length and step width-unified in a single optimization-based controller. We numerically validate our proposed method by demonstrating dynamic 3D walking at 0.6 m/s on DURUS, a 23 degree-of-freedom underactuated humanoid robot.},
	booktitle = {{IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	publisher = {IEEE},
	author = {Nguyen, Quan and Hereid, Ayonga and Grizzle, Jessy W. and Ames, Aaron D. and Sreenath, Koushil},
	month = dec,
	year = {2016},
	keywords = {control barrier functions, hybrid system, non-linear optimization},
	pages = {827--834},
}

@inproceedings{lin_learning_2015,
	address = {Seattle, WA, USA},
	title = {Learning null space projections},
	doi = {10.1109/ICRA.2015.7139551},
	abstract = {Many everyday human skills can be considered in terms of performing some task subject to a set of self-imposed or environmental constraints. In recent years, a number of new tools have become available in the learning and robotics community that allow data from constrained and/or redundant systems to be used to uncover underlying consistent behaviours that may be otherwise masked by the constraints. However, while a wide variety of work for generalisation of movements have been proposed, few have explicitly considered learning the constraints of the motion and ways to cope with unknown environment. In this paper, we propose a method to learn the constraints such that some previously learnt behaviours can be adapted to new environment in an appropriate way. In particular, we consider learning the null space projection matrix of a kinematically constrained system, and see how previously learnt policies can be adapted to novel constraints.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lin, Hsiu-Chin and Howard, Matthew and Vijayakumar, Sethu},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	pages = {2613--2619},
}

@inproceedings{lin_contact_2020,
	address = {Paris, France},
	title = {Contact {Surface} {Estimation} via {Haptic} {Perception}},
	doi = {10.1109/ICRA40945.2020.9196816},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lin, Hsiu-Chin and Mistry, Michael},
	month = may,
	year = {2020},
	pages = {5087--5093},
}

@inproceedings{haarnoja_learning_2019,
	address = {FreiburgimBreisgau, Germany},
	title = {Learning to walk via deep reinforcement learning},
	url = {https://openreview.net/forum?id=m1-SuAcV0K9},
	doi = {10.15607/RSS.2019.XV.011},
	language = {en},
	urldate = {2022-01-25},
	author = {Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
	month = jan,
	year = {2019},
	keywords = {deep reinforcement learning, maximum entropy RL, model-free, real world},
}

@inproceedings{berkenkamp_safe_2017,
	title = {Safe model-based reinforcement learning with stability guarantees},
	volume = {1705},
	url = {http://arxiv.org/abs/1705.08551},
	abstract = {Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.},
	urldate = {2022-01-28},
	booktitle = {Proceedings of {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela P. and Krause, Andreas},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.08551},
	keywords = {approximate dynamic programming, control barrier functions, deep reinforcement learning, lyapunov function, model-based, reinforcement learning},
}

@inproceedings{xiao_learning_2019,
	title = {Learning {Locomotion} {Skills} via {Model}-based {Proximal} {Meta}-{Reinforcement} {Learning}},
	doi = {10.1109/SMC.2019.8914406},
	abstract = {Model-based reinforcement learning methods provide a promising direction for a range of automated applications, such as autonomous vehicles and legged robots, due to their sample-efficiency. However, their asymptotic performance is usually inferior compared to the state-of-the-art model-free reinforcement learning methods in locomotion control domains. One main challenge of model-based reinforcement learning is learning a dynamics model that is accurate enough for planning. This paper mitigates this issue by meta-reinforcement learning from an ensemble of dynamics models. A policy learns from dynamics models that hold different beliefs of a real environment. This procedure improves its adaptability and inaccuracy-tolerance ability. A proximal meta-reinforcement learning algorithm is introduced to improve computational efficiency and reduces variance of higher-order gradient estimation. A heteroscedastic noise is added to the training dataset, thus leading to a robust and efficient model learning. Subsequently, proximal meta-reinforcement learning maximizes the expected returns by sampling “imaginary” trajectories from the learned dynamics, which does not require real environment data and can be deployed on many servers in parallel to speed up the whole learning process. The aim of this work is to reduce the sample-complexity and computational cost of reinforcement learning in robot locomotion tasks. Simulation experiments show that the proposed algorithm achieves an asymptotic performance compared with the state-of-the-art model-free reinforcement learning methods with significantly fewer samples, which confirm our theoretical results.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Systems}, {Man} and {Cybernetics} ({SMC})},
	author = {Xiao, Qing and Cao, Zhengcai and Zhou, Mengchu},
	month = oct,
	year = {2019},
	note = {ISSN: 2577-1655},
	keywords = {model-based, reinforcement learning},
	pages = {1545--1550},
}

@inproceedings{ozaln_implementation_2019,
	title = {An {Implementation} of {Vision} {Based} {Deep} {Reinforcement} {Learning} for {Humanoid} {Robot} {Locomotion}},
	doi = {10.1109/INISTA.2019.8778209},
	abstract = {Deep reinforcement learning (DRL) exhibits a promising approach for controlling humanoid robot locomotion. However, only values relating sensors such as IMU, gyroscope, and GPS are not sufficient robots to learn their locomotion skills. In this article, we aim to show the success of vision based DRL. We propose a new vision based deep reinforcement learning algorithm for the locomotion of the Robotis-op2 humanoid robot for the first time. In experimental setup, we construct the locomotion of humanoid robot in a specific environment in the Webots software. We use Double Dueling Q Networks (D3QN) and Deep Q Networks (DQN) that are a kind of reinforcement learning algorithm. We present the performance of vision based DRL algorithm on a locomotion experiment. The experimental results show that D3QN is better than DQN in that stable locomotion and fast training and the vision based DRL algorithms will be successfully able to use at the other complex environments and applications.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {INnovations} in {Intelligent} {SysTems} and {Applications} ({INISTA})},
	author = {Özaln, Recen and Kaymak, Cağri and Yildirum, Özal and Ucar, Ayşcgül and Demir, Yakup and Güzeliş, Cüneyt},
	month = jul,
	year = {2019},
	keywords = {deep Q network (DQN), deep reinforcement learning, vision},
	pages = {1--5},
}

@inproceedings{shi_maximum_2020,
	title = {Maximum entropy reinforcement learning with evolution strategies},
	doi = {10.1109/IJCNN48605.2020.9207570},
	abstract = {Evolution strategies (ES) have recently raised attention in solving challenging tasks with low computation costs and high scalability. However, it is well-known that evolution strategies reinforcement learning (RL) methods suffer from low stability. Without careful consideration, ES methods are sensitive to local optima and are unstable in learning. Therefore, there is an urgent need for improving the stability of ES methods in solving RL problems. In this paper, we propose a simple yet efficient ES method to stabilize the learning. Specifically, we propose a framework to incorporate the maximum entropy reinforcement learning with evolution strategies and derive an efficient entropy calculation method for linear policies. We further present a practical algorithm called maximum entropy evolution policy search based on the proposed framework, which is efficient and stable for policy search in continuous control. Our algorithm shows high stability across different random seeds and can obtain comparable results in performance against some existing derivative-free RL methods on several of the well-known benchmark MuJoCo robotic control tasks.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Shi, Longxiang and Li, Shijian and Zheng, Qian and Cao, Longbing and Yang, Long and Pan, Gang},
	month = jul,
	year = {2020},
	keywords = {deep reinforcement learning, maximum entropy RL, reinforcement learning},
	pages = {1--8},
}

@misc{eysenbach_maximum_2021,
	title = {Maximum entropy {RL} (provably) solves some robust {RL} problems},
	url = {http://arxiv.org/abs/2103.06257},
	abstract = {Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that standard maximum entropy RL is robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the ﬁrst rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require adding additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our theoretical results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modiﬁcations. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL does possess a striking simplicity and appealing formal guarantees.},
	language = {en},
	urldate = {2022-01-30},
	journal = {Berkeley Artificial Intelligence Research},
	author = {Eysenbach, Benjamin and Levine, Sergey},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.06257},
	keywords = {maximum entropy RL, reinforcement learning},
}

@article{fan_fully_2020,
	title = {Fully distributed multi-robot collision avoidance via deep reinforcement learning for safe and efficient navigation in complex scenarios},
	volume = {38},
	doi = {10.1177/0278364920916531},
	abstract = {A decentralized sensor-level collision-avoidance policy for multi-robot systems, which enables a robot to make effective progress in a crowd without getting stuck and has been successfully deployed on different types of physical robot platforms without tedious parameter tuning. Developing a safe and efficient collision-avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths with limited observation of other robots’ states and intentions. Prior distributed multi-robot collision-avoidance systems often require frequent inter-robot communication or agent-level features to plan a local collision-free action, which is not robust and computationally prohibitive. In addition, the performance of these methods is not comparable with their centralized counterparts in practice. In this article, we present a decentralized sensor-level collision-avoidance policy for multi-robot systems, which shows promising results in practical applications. In particular, our policy directly maps raw sensor measurements to an agent’s steering commands in terms of the movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots in rich, complex environments simultaneously using a policy-gradient-based reinforcement-learning algorithm. The learning algorithm is also integrated into a hybrid control framework to further improve the policy’s robustness and effectiveness. We validate the learned sensor-level collision-3avoidance policy in a variety of simulated and real-world scenarios with thorough performance evaluations for large-scale multi-robot systems. The generalization of the learned policy is verified in a set of unseen scenarios including the navigation of a group of heterogeneous robots and a large-scale scenario with 100 robots. Although the policy is trained using simulation data only, we have successfully deployed it on physical robots with shapes and dynamics characteristics that are different from the simulated agents, in order to demonstrate the controller’s robustness against the simulation-to-real modeling error. Finally, we show that the collision-avoidance policy learned from multi-robot navigation tasks provides an excellent solution for safe and effective autonomous navigation for a single robot working in a dense real human crowd. Our learned policy enables a robot to make effective progress in a crowd without getting stuck. More importantly, the policy has been successfully deployed on different types of physical robot platforms without tedious parameter tuning. Videos are available at https://sites.google.com/view/hybridmrca.},
	journal = {The International Journal of Robotics Research},
	author = {Fan, Tingxiang and Long, Pinxin and Liu, Wenxi and Pan, Jia},
	year = {2020},
	keywords = {deep reinforcement learning, sim2real, simulation},
	pages = {856--892},
}

@article{lee_robust_2019,
	title = {Robust recovery controller for a quadrupedal robot using deep reinforcement learning},
	abstract = {This paper presents an approach based on model-free Deep Reinforcement Learning to control recovery maneuvers of quadrupedal robots using a hierarchical behavior-based controller that manifests dynamic and reactive recovery behaviors to recover from an arbitrary fall configuration within less than 5 seconds. The ability to recover from a fall is an essential feature for a legged robot to navigate in challenging environments robustly. Until today, there has been very little progress on this topic. Current solutions mostly build upon (heuristically) predefined trajectories, resulting in unnatural behaviors and requiring considerable effort in engineering system-specific components. In this paper, we present an approach based on model-free Deep Reinforcement Learning (RL) to control recovery maneuvers of quadrupedal robots using a hierarchical behavior-based controller. The controller consists of four neural network policies including three behaviors and one behavior selector to coordinate them. Each of them is trained individually in simulation and deployed directly on a real system. We experimentally validate our approach on the quadrupedal robot ANYmal, which is a dog-sized quadrupedal system with 12 degrees of freedom. With our method, ANYmal manifests dynamic and reactive recovery behaviors to recover from an arbitrary fall configuration within less than 5 seconds. We tested the recovery maneuver more than 100 times, and the success rate was higher than 97 \%.},
	journal = {ArXiv},
	author = {Lee, Joonho and Hwangbo, Jemin and Hutter, M.},
	year = {2019},
	keywords = {actuator model (s2r), anymal, deep reinforcement learning, domain randomization (s2r), no safety, noisy input (s2r), sim2real, simulation, trust region policy optimization (TRPO)},
}

@inproceedings{chow_safe_2020,
	title = {Safe policy learning for continuous control},
	url = {https://drive.google.com/file/d/1sBMJA0ekiqiNIYptNM8uqv3oapm9UW4z/view},
	urldate = {2022-01-26},
	author = {Chow, Yinlam and Nachum, Ofir and Faust, Aleksandra and Guzman, Edgar Duenez and Ghavamzadeh, Mohammad},
	year = {2020},
	keywords = {lyapunov function, simulation},
}

@article{lee_learning_2020,
	title = {Learning quadrupedal locomotion over challenging terrain},
	volume = {5},
	issn = {2470-9476},
	url = {http://arxiv.org/abs/2010.11251},
	doi = {10.1126/scirobotics.abc5986},
	abstract = {Some of the most challenging environments on our planet are accessible to quadrupedal animals but remain out of reach for autonomous machines. Legged locomotion can dramatically expand the operational domains of robotics. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have escalated in complexity while falling short of the generality and robustness of animal locomotion. Here we present a radically robust controller for legged locomotion in challenging natural environments. We present a novel solution to incorporating proprioceptive feedback in locomotion control and demonstrate remarkable zero-shot generalization from simulation to natural environments. The controller is trained by reinforcement learning in simulation. It is based on a neural network that acts on a stream of proprioceptive signals. The trained controller has taken two generations of quadrupedal ANYmal robots to a variety of natural environments that are beyond the reach of prior published work in legged locomotion. The controller retains its robustness under conditions that have never been encountered during training: deformable terrain such as mud and snow, dynamic footholds such as rubble, and overground impediments such as thick vegetation and gushing water. The presented work opens new frontiers for robotics and indicates that radical robustness in natural environments can be achieved by training in much simpler domains.},
	number = {47},
	urldate = {2022-01-05},
	journal = {Science Robotics},
	author = {Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11251},
	keywords = {learned dynamics (s2r), privileged learning, sim2real, simulation},
}

@article{miki_learning_2022,
	title = {Learning robust perceptive locomotion for quadrupedal robots in the wild},
	volume = {7},
	copyright = {Copyright © 2022 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
	url = {https://www.science.org/doi/abs/10.1126/scirobotics.abk2822},
	doi = {10.1126/scirobotics.abk2822},
	abstract = {A legged locomotion controller achieves high robustness and speed in the wild by combining multimodal information.},
	language = {EN},
	number = {67},
	urldate = {2022-01-28},
	journal = {Science Robotics},
	author = {Miki, Takahiro and Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
	month = jan,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {actuator model (s2r), parallel D-RL, privileged learning, sim2real, simulation},
}

@inproceedings{singla_realizing_2019,
	title = {Realizing learned quadruped locomotion behaviors through kinematic motion primitives},
	doi = {10.1109/ICRA.2019.8794179},
	abstract = {This paper realizes walking in Stoch by a straightforward reconstruction of joint trajectories from kMPs, which improves the transferability of these gaits to real hardware, lowers the computational overhead on-board, and also avoids multiple training iterations by generating a set of derived behaviors from a single learned gait. Humans and animals are believed to use a very minimal set of trajectories to perform a wide variety of tasks including walking. Our main objective in this paper is two fold 1) Obtain an effective tool to realize these basic motion patterns for quadrupedal walking, called the kinematic motion primitives (kMPs), via trajectories learned from deep reinforcement learning (D-RL) and 2) Realize a set of behaviors, namely trot, walk, gallop and bound from these kinematic motion primitives in our custom four legged robot, called the “Stoch”. D-RL is a data driven approach, which has been shown to be very effective for realizing all kinds of robust locomotion behaviors, both in simulation and in experiment. On the other hand, kMPs are known to capture the underlying structure of walking and yield a set of derived behaviors. We first generate walking gaits from D-RL, which uses policy gradient based approaches. We then analyze the resulting walking by using principal component analysis. We observe that the kMPs extracted from PCA followed a similar pattern irrespective of the type of gaits generated. Leveraging on this underlying structure, we then realize walking in Stoch by a straightforward reconstruction of joint trajectories from kMPs. This type of methodology improves the transferability of these gaits to real hardware, lowers the computational overhead on-board, and also avoids multiple training iterations by generating a set of derived behaviors from a single learned gait.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Singla, Abhik and Bhattacharya, S. and Dholakiya, Dhaivat and Bhatnagar, S. and Ghosal, A. and Amrutur, B. and Kolathaya, Shishir},
	year = {2019},
	keywords = {sim2real},
	pages = {7434--7440},
}

@inproceedings{nguyen_optimal_2015,
	title = {Optimal robust control for bipedal robots through control lyapunov function based quadratic programs},
	doi = {10.15607/RSS.2015.XI.048},
	author = {Nguyen, Quan and Sreenath, Koushil},
	month = jul,
	year = {2015},
	keywords = {lyapunov function},
}

@inproceedings{grandia_multi-layered_2021,
	title = {Multi-layered safety for legged robots via control barrier functions and model predictive control},
	doi = {10.1109/ICRA48506.2021.9561510},
	abstract = {The problem of dynamic locomotion over rough terrain requires both accurate foot placement together with an emphasis on dynamic stability. Existing approaches to this problem prioritize immediate safe foot placement over longer term dynamic stability considerations, or relegate the coordination of foot placement and dynamic stability to heuristic methods. We propose a multi-layered locomotion framework that unifies Control Barrier Functions (CBFs) with Model Predictive Control (MPC) to simultaneously achieve safe foot placement and dynamic stability. Our approach incorporates CBF based safety constraints both in a low frequency kinodynamic MPC formulation and a high frequency inverse dynamics tracking controller. This ensures that safety-critical execution is considered when optimizing locomotion over a longer horizon. We validate the proposed method in a 3D stepping-stone scenario in simulation and experimentally on the ANYmal quadruped platform.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Grandia, Ruben and Taylor, Andrew J. and Ames, Aaron D. and Hutter, Marco},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {control barrier functions},
	pages = {8352--8358},
}

@article{ames_control_2017,
	title = {Control barrier function based quadratic programs for safety critical systems},
	volume = {62},
	issn = {0018-9286, 1558-2523},
	url = {http://arxiv.org/abs/1609.06408},
	doi = {10.1109/TAC.2016.2638961},
	abstract = {Safety critical systems involve the tight coupling between potentially conflicting control objectives and safety constraints. As a means of creating a formal framework for controlling systems of this form, and with a view toward automotive applications, this paper develops a methodology that allows safety conditions -- expressed as control barrier functions -- to be unified with performance objectives -- expressed as control Lyapunov functions -- in the context of real-time optimization-based controllers. Safety conditions are specified in terms of forward invariance of a set, and are verified via two novel generalizations of barrier functions; in each case, the existence of a barrier function satisfying Lyapunov-like conditions implies forward invariance of the set, and the relationship between these two classes of barrier functions is characterized. In addition, each of these formulations yields a notion of control barrier function (CBF), providing inequality constraints in the control input that, when satisfied, again imply forward invariance of the set. Through these constructions, CBFs can naturally be unified with control Lyapunov functions (CLFs) in the context of a quadratic program (QP); this allows for the achievement of control objectives (represented by CLFs) subject to conditions on the admissible states of the system (represented by CBFs). The mediation of safety and performance through a QP is demonstrated on adaptive cruise control and lane keeping, two automotive control problems that present both safety and performance considerations coupled with actuator bounds.},
	number = {8},
	urldate = {2021-12-08},
	journal = {IEEE Transactions on Automatic Control},
	author = {Ames, Aaron D. and Xu, Xiangru and Grizzle, Jessy W. and Tabuada, Paulo},
	month = aug,
	year = {2017},
	note = {arXiv: 1609.06408},
	keywords = {control barrier functions},
	pages = {3861--3876},
}

@article{ames_control_2019,
	title = {Control barrier functions: {Theory} and applications},
	shorttitle = {Control {Barrier} {Functions}},
	url = {http://arxiv.org/abs/1903.11199},
	abstract = {This paper provides an introduction and overview of recent work on control barrier functions and their use to verify and enforce safety properties in the context of (optimization based) safety-critical controllers. We survey the main technical results and discuss applications to several domains including robotic systems.},
	urldate = {2022-01-25},
	journal = {arXiv:1903.11199 [cs]},
	author = {Ames, Aaron D. and Coogan, Samuel and Egerstedt, Magnus and Notomista, Gennaro and Sreenath, Koushil and Tabuada, Paulo},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.11199},
	keywords = {control barrier functions},
}

@article{chow_lyapunov-based_2018,
	series = {{NIPS}'18},
	title = {A lyapunov-based approach to safe reinforcement learning},
	url = {http://arxiv.org/abs/1805.07708},
	abstract = {In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance it is crucial to guarantee the safety of an agent during training as well as deployment (e.g. a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision problems (CMDPs), an extension of the standard Markov decision problems (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel {\textbackslash}emph\{Lyapunov\} method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.},
	urldate = {2022-01-25},
	journal = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	author = {Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
	month = dec,
	year = {2018},
	note = {arXiv: 1805.07708},
	keywords = {lyapunov function},
	pages = {8103--8112},
}

@inproceedings{cheng_end--end_2019,
	title = {End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks},
	doi = {10.1609/aaai.v33i01.33013387},
	abstract = {This work proposes a controller architecture that combines a model-free RL-based controller with model-based controllers utilizing control barrier functions (CBFs) and on-line learning of the unknown system dynamics, in order to ensure safety during learning. Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) online learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties. 
Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous carfollowing with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.},
	booktitle = {{AAAI}},
	author = {Cheng, Richard and Orosz, G. and Murray, R. and Burdick, J.},
	year = {2019},
	keywords = {lyapunov function},
}

@inproceedings{hafner_towards_2020,
	title = {Towards general and autonomous learning of core skills: {A} case study in locomotion},
	shorttitle = {Towards {General} and {Autonomous} {Learning} of {Core} {Skills}},
	abstract = {This paper develops a learning framework that can learn sophisticated locomotion behavior for a wide spectrum of legged robots, such as bipeds, tripeds, quadrupeds and hexapods, including wheeled variants and demonstrates that the same algorithm can rapidly learn diverse and reusable locomotion skills without any platform specific adjustments or additional instrumentation of the learning setup. Modern Reinforcement Learning (RL) algorithms promise to solve difficult motor control problems directly from raw sensory inputs. Their attraction is due in part to the fact that they can represent a general class of methods that allow to learn a solution with a reasonably set reward and minimal prior knowledge, even in situations where it is difficult or expensive for a human expert. For RL to truly make good on this promise, however, we need algorithms and learning setups that can work across a broad range of problems with minimal problem specific adjustments or engineering. In this paper, we study this idea of generality in the locomotion domain. We develop a learning framework that can learn sophisticated locomotion behavior for a wide spectrum of legged robots, such as bipeds, tripeds, quadrupeds and hexapods, including wheeled variants. Our learning framework relies on a data-efficient, off-policy multi-task RL algorithm and a small set of reward functions that are semantically identical across robots. To underline the general applicability of the method, we keep the hyper-parameter settings and reward definitions constant across experiments and rely exclusively on on-board sensing. For nine different types of robots, including a real-world quadruped robot, we demonstrate that the same algorithm can rapidly learn diverse and reusable locomotion skills without any platform specific adjustments or additional instrumentation of the learning setup.},
	booktitle = {{CoRL}},
	author = {Hafner, Roland and Hertweck, Tim and Kloppner, Philipp and Bloesch, Michael and Neunert, Michael and Wulfmeier, Markus and Tunyasuvunakool, S. and Heess, N. and Riedmiller, Martin A.},
	year = {2020},
	keywords = {deep reinforcement learning, multi-task RL, simulation},
}

@article{le_probabilistic_2021,
	title = {Probabilistic {Surface} {Friction} {Estimation} {Based} on {Visual} and {Haptic} {Measurements}},
	volume = {6},
	issn = {2377-3766},
	doi = {10.1109/LRA.2021.3062585},
	abstract = {Accurately modeling local surface properties of objects is crucial to many robotic applications, from grasping to material recognition. Surface properties like friction are however difficult to estimate, as visual observation of the object does not convey enough information over these properties. In contrast, haptic exploration is time consuming as it only provides information relevant to the explored parts of the object. In this letter, we propose a joint visuo-haptic object model that enables the estimation of surface friction coefficient over an entire object by exploiting the correlation of visual and haptic information, together with a limited haptic exploration by a robotic arm. We demonstrate the validity of the proposed method by showing its ability to estimate varying friction coefficients on a range of real multi-material objects. Furthermore, we illustrate how the estimated friction coefficients can improve grasping success rate by guiding a grasp planner toward high friction areas.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Le, Tran Nguyen and Verdoja, Francesco and Abu-Dakka, Fares J. and Kyrki, Ville},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	pages = {2838--2845},
}

@inproceedings{ha_learning_2020,
	title = {Learning to walk in the real world with minimal human effort},
	abstract = {This paper develops a system for learning legged locomotion policies with deep RL in the real world with minimal human effort by developing a multi-task learning procedure, an automatic reset controller, and a safety-constrained RL framework. Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure, an automatic reset controller, and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention.},
	booktitle = {Conference on {Robot} {Learning}},
	author = {Ha, Sehoon and Xu, Peng and Tan, Zhenyu and Levine, Sergey and Tan, Jie},
	year = {2020},
	keywords = {actor critic, deep reinforcement learning, real world, safety as constraint, soft actor critic},
}

@article{nguyen_safety-critical_2015,
	series = {Analysis and {Design} of {Hybrid} {Systems} {ADHS}},
	title = {Safety-critical control for dynamical bipedal walking with precise footstep placement},
	volume = {48},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896315024258},
	doi = {10.1016/j.ifacol.2015.11.167},
	abstract = {This paper presents a novel methodology to achieve dynamic walking for underactu-ated and hybrid dynamical bipedal robots subject to safety-critical position-based constraints. The proposed controller is based on the combination of control Barrier functions and control Lyapunov functions implemented as a state-based online quadratic program to achieve stability under input and state constraints, while simultaneously enforcing safety. The main contribution of this paper is the control design to enable stable dynamical bipedal walking subject to strict safety constraints that arise due to walking over a terrain with randomly generated discrete footholds and overhead obstacles. Evaluation of our proposed control design is presented on a model of RABBIT, a fve-link planar underacted bipedal robot with point feet.},
	language = {en},
	number = {27},
	urldate = {2022-01-25},
	journal = {IFAC Proceedings Volumes},
	author = {Nguyen, Quan and Sreenath, Koushil},
	month = jan,
	year = {2015},
	keywords = {control barrier functions, lyapunov function, quadratic program},
	pages = {147--154},
}

@inproceedings{akametalu_reachability-based_2014,
	title = {Reachability-based safe learning with {Gaussian} processes},
	doi = {10.1109/CDC.2014.7039601},
	abstract = {Reinforcement learning for robotic applications faces the challenge of constraint satisfaction, which currently impedes its application to safety critical systems. Recent approaches successfully introduce safety based on reachability analysis, determining a safe region of the state space where the system can operate. However, overly constraining the freedom of the system can negatively affect performance, while attempting to learn less conservative safety constraints might fail to preserve safety if the learned constraints are inaccurate. We propose a novel method that uses a principled approach to learn the system's unknown dynamics based on a Gaussian process model and iteratively approximates the maximal safe set. A modified control strategy based on real-time model validation preserves safety under weaker conditions than current approaches. Our framework further incorporates safety into the reinforcement learning performance metric, allowing a better integration of safety and learning. We demonstrate our algorithm on simulations of a cart-pole system and on an experimental quadrotor application and show how our proposed scheme succeeds in preserving safety where current approaches fail to avoid an unsafe condition.},
	booktitle = {53rd {IEEE} {Conference} on {Decision} and {Control}},
	author = {Akametalu, Anayo K. and Fisac, Jaime F. and Gillula, Jeremy H. and Kaynama, Shahab and Zeilinger, Melanie N. and Tomlin, Claire J.},
	month = dec,
	year = {2014},
	note = {ISSN: 0191-2216},
	pages = {1424--1431},
}

@article{perkins_lyapunov_2003,
	title = {Lyapunov design for safe reinforcement learning},
	volume = {3},
	issn = {1532-4435},
	abstract = {Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.},
	journal = {The Journal of Machine Learning Research},
	author = {Perkins, Theodore J. and Barto, Andrew G.},
	month = mar,
	year = {2003},
	keywords = {lyapunov function},
	pages = {803--832},
}

@inproceedings{homberger_support_2019,
	title = {Support {Surface} {Estimation} for {Legged} {Robots}},
	doi = {10.1109/ICRA.2019.8793646},
	abstract = {The high agility of legged systems allows them to operate in rugged outdoor environments. In these situations, knowledge about the terrain geometry is key for foothold planning to enable safe locomotion. However, on penetrable or highly compliant terrain (e.g. grass) the visibility of the supporting ground surface is obstructed, i.e. it cannot directly be perceived by depth sensors. We present a method to estimate the underlying terrain topography by fusing haptic information about foot contact closure locations with exteroceptive sensing. To obtain a dense support surface estimate from sparsely sampled footholds we apply Gaussian process regression. Exteroceptive information is integrated into the support surface estimation procedure by estimating the height of the penetrable surface layer from discrete penetration depth measurements at the footholds. The method is designed such that it provides a continuous support surface estimate even if there is only partial exteroceptive information available due to shadowing effects. Field experiments with the quadrupedal robot ANYmal show how the robot can smoothly and safely navigate in dense vegetation.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Homberger, Timon and Wellhausen, Lorenz and Fankhauser, Péter and Hutter, Marco},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	pages = {8470--8476},
}

@inproceedings{belter_single-shot_2019,
	title = {Single-shot {Foothold} {Selection} and {Constraint} {Evaluation} for {Quadruped} {Locomotion}},
	doi = {10.1109/ICRA.2019.8793801},
	abstract = {In this paper, we propose a method for selecting the optimal footholds for legged systems. The goal of the proposed method is to find the best foothold for the swing leg on a local elevation map. First, we evaluate the geometrical characteristics of each cell on the elevation map, checks kinematic constraints and collisions. Then, we apply the Convolutional Neural Network to learn the relationship between the local elevation map and the quality of potential footholds. During execution time, the controller obtains the qualitative measurement of each potential foothold from the neural model. This method evaluates hundreds of potential footholds and checks multiple constraints in a single step which takes 10 ms on a standard computer without GPU. The experiments were carried out on a quadruped robot walking over rough terrain in both simulation and real robotic platforms.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Belter, Dominik and Bednarek, Jakub and Lin, Hsiu-Chin and Xin, Guiyang and Mistry, Michael},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	pages = {7441--7447},
}

@inproceedings{tennakoon_probe-before-step_2020,
	title = {Probe-before-step walking strategy for multi-legged robots on terrain with risk of collapse},
	doi = {10.1109/ICRA40945.2020.9197154},
	abstract = {Multi-legged robots are effective at traversing rough terrain. However, terrains that include collapsible footholds (i.e. regions that can collapse when stepped on) remain a significant challenge, especially since such situations can be extremely difficult to anticipate using only exteroceptive sensing. State-of-the-art methods typically use various stabilisation techniques to regain balance and counter changing footholds. However, these methods are likely to fail if safe footholds are sparse and spread out or if the robot does not respond quickly enough after a foothold collapse. This paper presents a novel method for multi-legged robots to probe and test the terrain for collapses using its legs while walking. The proposed method improves on existing terrain probing approaches, and integrates the probing action into a walking cycle. A follow-the-leader strategy with a suitable gait and stance is presented and implemented on a hexapod robot. The proposed method is experimentally validated, demonstrating the robot can safely traverse terrain containing collapsible footholds.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Tennakoon, Eranda and Peynot, Thierry and Roberts, Jonathan and Kottege, Navinda},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	pages = {5530--5536},
}

@article{hwangbo_learning_2019,
	title = {Learning agile and dynamic motor skills for legged robots},
	volume = {4},
	issn = {2470-9476},
	url = {http://arxiv.org/abs/1901.08652},
	doi = {10.1126/scirobotics.aau5872},
	abstract = {Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we introduce a method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby leveraging fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated medium-dog-sized quadrupedal system. Using policies trained in simulation, the quadrupedal machine achieves locomotion skills that go beyond what had been achieved with prior methods: ANYmal is capable of precisely and energy-efficiently following high-level body velocity commands, running faster than before, and recovering from falling even in complex configurations.},
	language = {en},
	number = {26},
	urldate = {2022-01-29},
	journal = {Science Robotics},
	author = {Hwangbo, Jemin and Lee, Joonho and Dosovitskiy, Alexey and Bellicoso, Dario and Tsounis, Vassilios and Koltun, Vladlen and Hutter, Marco},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.08652},
	keywords = {deep reinforcement learning, simulation},
}

@article{nansai_novel_2015,
	title = {A novel approach to gait synchronization and transition for reconfigurable walking platforms},
	volume = {1},
	issn = {2352-8648},
	url = {https://www.sciencedirect.com/science/article/pii/S2352864815000231},
	doi = {10.1016/j.dcan.2015.04.003},
	abstract = {Legged robots based on one degree-of-freedom reconfigurable planar leg mechanisms, that are capable of generating multiple useful gaits, are highly desired due to the possibility of handling environments and tasks of high complexity while maintaining simple control schemes. An essential consideration in these reconfigurable legged robots is to attain stability in motion, at rest as well as while transforming from one configuration to another with the minimum number of legs as long as the full range of their walking patterns, resulting from the different gait cycles of their legs, is achieved. To this end, in this paper, we present a method for the generation of input joint trajectories to properly synchronize the movement of quadruped robots with reconfigurable legs. The approach is exemplified in a four-legged robot with reconfigurable Jansen legs capable of generating up to six useful different gait cycles. The proposed technique is validated through simulated results that show the platform׳s stability across its six feasible walking patterns and during gait transition phases, thus considerably extending the capabilities of the non-reconfigurable design.},
	language = {en},
	number = {2},
	urldate = {2022-01-07},
	journal = {Digital Communications and Networks},
	author = {Nansai, Shunsuke and Rojas, Nicolas and Elara, Mohan Rajesh and Sosa, Ricardo and Iwase, Masami},
	month = apr,
	year = {2015},
	pages = {141--151},
}

@article{fahmi_passive_2019,
	title = {Passive {Whole}-body {Control} for {Quadruped} {Robots}: {Experimental} {Validation} over {Challenging} {Terrain}},
	shorttitle = {Passive {Whole}-body {Control} for {Quadruped} {Robots}},
	url = {http://arxiv.org/abs/1811.00884},
	abstract = {We present experimental results using a passive whole-body control approach for quadruped robots that achieves dynamic locomotion while compliantly balancing the robot's trunk. We formulate the motion tracking as a Quadratic Program (QP) that takes into account the full robot rigid body dynamics, the actuation limits, the joint limits and the contact interaction. We analyze the controller's robustness against inaccurate friction coefficient estimates and unstable footholds, as well as its capability to redistribute the load as a consequence of enforcing actuation limits. Additionally, we present practical implementation details gained from the experience with the real platform. Extensive experimental trials on the 90 kg Hydraulically actuated Quadruped (HyQ) robot validate the capabilities of this controller under various terrain conditions and gaits. The proposed approach is superior for accurate execution of highly dynamic motions with respect to the current state of the art.},
	urldate = {2022-01-12},
	journal = {Robotics and Automation Letters},
	author = {Fahmi, Shamel and Mastalli, Carlos and Focchi, Michele and Semini, Claudio},
	month = mar,
	year = {2019},
	note = {arXiv: 1811.00884},
	keywords = {whole body control},
}

@article{zhao_sim--real_2020,
	title = {Sim-to-real transfer in deep reinforcement learning for robotics: {A} survey},
	shorttitle = {Sim-to-{Real} {Transfer} in {Deep} {Reinforcement} {Learning} for {Robotics}},
	url = {http://arxiv.org/abs/2009.13303},
	doi = {10.1109/SSCI47803.2020.9308468.},
	abstract = {Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefﬁciency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially inﬁnite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-toreal gap and accomplish more efﬁcient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.},
	language = {en},
	urldate = {2022-01-26},
	journal = {2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
	author = {Zhao, Wenshuai and Queralta, Jorge Peña and Westerlund, Tomi},
	month = dec,
	year = {2020},
	note = {arXiv: 2009.13303},
	keywords = {sim2real},
	pages = {737--744},
}

@article{peng_sim--real_2018,
	title = {Sim-to-real transfer of robotic control with dynamics randomization},
	url = {http://arxiv.org/abs/1710.06537},
	doi = {10.1109/ICRA.2018.8460528},
	abstract = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.},
	urldate = {2022-01-26},
	journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
	author = {Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
	month = may,
	year = {2018},
	note = {arXiv: 1710.06537},
	keywords = {sim2real},
	pages = {3803--3810},
}

@article{tan_sim--real_2018,
	title = {Sim-to-{Real}: {Learning} {Agile} {Locomotion} {For} {Quadruped} {Robots}},
	shorttitle = {Sim-to-{Real}},
	url = {http://arxiv.org/abs/1804.10332},
	abstract = {Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.},
	urldate = {2022-01-29},
	journal = {Robotics: Science and Systems},
	author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
	month = may,
	year = {2018},
	note = {arXiv: 1804.10332},
	keywords = {sim2real},
}

@incollection{neunert_why_2016,
	title = {Why off-the-shelf physics simulators fail in evaluating feedback controller performance - a case study for quadrupedal robots},
	isbn = {978-981-314-912-0},
	url = {https://www.worldscientific.com/doi/abs/10.1142/9789813149137_0055},
	urldate = {2022-01-29},
	booktitle = {Advances in {Cooperative} {Robotics}},
	publisher = {WORLD SCIENTIFIC},
	author = {Neunert, M. and Boaventura, T. and Buchli, J.},
	month = jul,
	year = {2016},
	doi = {10.1142/9789813149137_0055},
	keywords = {sim2real},
	pages = {464--472},
}

@article{kaspar_sim2real_2020,
	title = {Sim2real transfer for reinforcement learning without dynamics randomization},
	url = {http://arxiv.org/abs/2002.11635},
	abstract = {In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa peg in-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot.},
	urldate = {2022-01-26},
	journal = {arXiv:2002.11635 [cs]},
	author = {Kaspar, Manuel and Osorio, Juan David Munoz and Bock, Jürgen},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.11635},
	keywords = {sim2real},
}

@inproceedings{yang_data_2020,
	address = {Osaka, Japan},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Data efficient reinforcement learning for legged robots},
	volume = {100},
	url = {http://arxiv.org/abs/1907.03613},
	abstract = {We present a model-based framework for robot locomotion that achieves walking based on only 4.5 minutes (45,000 control steps) of data collected on a quadruped robot. To accurately model the robot's dynamics over a long horizon, we introduce a loss function that tracks the model's prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function. To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.},
	language = {en},
	urldate = {2022-01-25},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Yang, Yuxiang and Caluwaerts, Ken and Iscen, Atil and Zhang, Tingnan and Tan, Jie and Sindhwani, Vikas},
	month = nov,
	year = {2020},
	note = {arXiv: 1907.03613},
	keywords = {real world},
	pages = {1--10},
}

@article{tsounis_deepgait_2020,
	title = {{DeepGait}: {Planning} and control of quadrupedal gaits using deep reinforcement learning},
	volume = {5},
	shorttitle = {{DeepGait}},
	url = {http://arxiv.org/abs/1909.08399},
	doi = {10.1109/LRA.2020.2979660},
	abstract = {This paper addresses the problem of legged locomotion in non-flat terrain. As legged robots such as quadrupeds are to be deployed in terrains with geometries which are difficult to model and predict, the need arises to equip them with the capability to generalize well to unforeseen situations. In this work, we propose a novel technique for training neural-network policies for terrain-aware locomotion, which combines state-of-the-art methods for model-based motion planning and reinforcement learning. Our approach is centered on formulating Markov decision processes using the evaluation of dynamic feasibility criteria in place of physical simulation. We thus employ policy-gradient methods to independently train policies which respectively plan and execute foothold and base motions in 3D environments using both proprioceptive and exteroceptive measurements. We apply our method within a challenging suite of simulated terrain scenarios which contain features such as narrow bridges, gaps and stepping-stones, and train policies which succeed in locomoting effectively in all cases.},
	number = {2},
	urldate = {2022-01-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tsounis, Vassilios and Alge, Mitja and Lee, Joonho and Farshidian, Farbod and Hutter, Marco},
	month = mar,
	year = {2020},
	note = {arXiv: 1909.08399},
	keywords = {simulation},
	pages = {3699--3706},
}

@inproceedings{peng_learning_2020,
	title = {Learning agile robotic locomotion skills by imitating animals},
	doi = {10.15607/RSS.2020.XVI.064},
	author = {Peng, Xue and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang-Wei and Tan, Jie and Levine, Sergey},
	month = jul,
	year = {2020},
	keywords = {domain randomization (s2r), imitation learning, sim2real, simulation},
}

@article{winkler_gait_2018,
	title = {Gait and {Trajectory} {Optimization} for {Legged} {Systems} {Through} {Phase}-{Based} {End}-{Effector} {Parameterization}},
	volume = {3},
	issn = {2377-3766, 2377-3774},
	url = {http://ieeexplore.ieee.org/document/8283570/},
	doi = {10.1109/LRA.2018.2798285},
	abstract = {We present a single Trajectory Optimization formulation for legged locomotion that automatically determines the gait-sequence, step-timings, footholds, swing-leg motions and 6D body motion over non-ﬂat terrain, without any additional modules. Our phase-based parameterization of feet motion and forces allows to optimize over the discrete gait sequence using only continuous decision variables. The system is represented using a simpliﬁed Centroidal dynamics model that is inﬂuenced by the feet’s location and forces. We explicitly enforce friction cone constraints, depending on the shape of the terrain. The NLP solver generates highly dynamic motion-plans with full ﬂight-phases for a variety of legged systems with arbitrary morphologies in an efﬁcient manner. We validate the feasibility of the generated plans in simulation and on the real quadruped robot ANYmal. Additionally, the entire solver software TOWR used to generate these motions is made freely available.},
	language = {en},
	number = {3},
	urldate = {2022-01-29},
	journal = {IEEE Robotics and Automation Letters},
	author = {Winkler, Alexander W. and Bellicoso, C. Dario and Hutter, Marco and Buchli, Jonas},
	month = jul,
	year = {2018},
	keywords = {trajectory optimization},
	pages = {1560--1567},
}

@article{li_planning_2021,
	title = {Planning in learned latent action spaces for generalizable legged locomotion},
	volume = {PP},
	doi = {10.1109/LRA.2021.3062342},
	abstract = {Hierarchical learning has been successful at learning generalizable locomotion skills on walking robots in a sample-efficient manner. However, the low-dimensional ‘'latent’' action used to communicate between two layers of the hierarchy is typically user-designed. In this work, we present a fully-learned hierarchical framework, that is capable of jointly learning the low-level controller and the high-level latent action space. Once this latent space is learned, we plan over continuous latent actions in a model-predictive control fashion, using a learned high-level dynamics model. This framework generalizes to multiple robots, and we present results on a Daisy hexapod simulation, A1 quadruped simulation, and Daisy robot hardware. We compare a range of learned hierarchical approaches from literature, and show that our framework outperforms baselines on multiple tasks and two simulations. In addition to learning approaches, we also compare to inverse-kinematics (IK) acting on desired robot motion, and show that our fully-learned framework outperforms IK in adverse settings on both A1 and Daisy simulations. On hardware, we show the Daisy hexapod achieve multiple locomotion tasks, in an unstructured outdoor setting, with only 2000 hardware samples, reinforcing the robustness and sample-efficiency of our approach.},
	journal = {IEEE Robotics and Automation Letters},
	author = {Li, Tianyu and Calandra, Roberto and Pathak, Deepak and Tian, Yuandong and Meier, Franziska and Rai, Akshara},
	month = feb,
	year = {2021},
	keywords = {simulation},
	pages = {1--1},
}

@inproceedings{rudin_learning_2021,
	title = {Learning to walk in minutes using massively parallel deep reinforcement learning},
	url = {https://openreview.net/forum?id=wK2fDDJ5VcF},
	abstract = {In this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU. We analyze and...},
	language = {en},
	urldate = {2022-01-26},
	author = {Rudin, Nikita and Hoeller, David and Reist, Philipp and Hutter, Marco},
	month = jun,
	year = {2021},
	keywords = {deep reinforcement learning, parallel D-RL, simulation},
}

@article{focchi_high-slope_2017,
	title = {High-slope {Terrain} {Locomotion} for {Torque}-{Controlled} {Quadruped} {Robots}},
	volume = {41},
	url = {https://hal.archives-ouvertes.fr/hal-01137225},
	doi = {10.1007/s10514-016-9573-1},
	abstract = {Research into legged robotics is primarily motivated by the prospects of building machines that are able to navigate in challenging and complex environments that are predominantly non-flat. In this context, control of contact forces is fundamental to ensure stable contacts and equilibrium of the robot. In this paper we propose a planning/control framework for quasi-static walking of quadrupedal robots, implemented for a demanding application in which regulation of ground reaction forces is crucial. Experimental results demonstrate that our 75-kg quadruped robot is able to  walk inside two high-slope (50 degrees) V-shaped walls; an achievement that to the authors' best knowledge has never been presented before. The robot distributes its weight among the stance legs so as to optimize user-defined criteria. We compute joint torques that result in no foot slippage, fulfillment of the unilateral constraints of the contact forces and minimization of the actuators effort. The presented study  is an experimental validation of the effectiveness and robustness of QP-based force distributions methods for quasi-static locomotion on challenging terrain.},
	number = {1},
	urldate = {2022-01-12},
	journal = {Autonomous Robots},
	author = {Focchi, Michele and Del Prete, Andrea and Havoutis, Ioannis and Featherstone, Roy and Caldwell, Darwin and Semini, Claudio},
	month = jan,
	year = {2017},
	note = {Publisher: Springer Verlag},
	pages = {259--272},
}
